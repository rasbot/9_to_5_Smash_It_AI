{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- PROJECT SHIELDS -->\n",
    "<!-- [![LinkedIn][linkedin-shield]][linkedin-url] -->\n",
    "\n",
    "<img src=\"images/robot_fail.gif\" alt=\"Logo\" width=\"400\" height=\"300\">\n",
    "\n",
    "# Reinforcement Learning in 3D Simulated Environments\n",
    "\n",
    "<!-- PROJECT LOGO -->\n",
    "<br />\n",
    "<!-- <p align=\"center\">\n",
    "  <a href=\"https://github.com/rasbot/Reinforcement_Learning_in_Unity\">\n",
    "    <img src=\"images/robot_fail.gif\" alt=\"Logo\" width=\"600\" height=\"400\">\n",
    "  </a>\n",
    "\n",
    "  <h3 align=\"center\">Reinforcement Learning in 3D Simulated Environments</h3> -->\n",
    "\n",
    "<!--   <p align=\"center\">\n",
    "    Using Unity to simulate environments to train agents using reinforcement learning.\n",
    "    <br />\n",
    "    <a href=\"https://github.com/rasbot/Reinforcement_Learning_in_Unity\"><strong>Explore the docs »</strong></a>\n",
    "    <br />\n",
    "    <br />\n",
    "    <a href=\"https://github.com/rasbot/Reinforcement_Learning_in_Unity\">View Demo</a>\n",
    "    ·\n",
    "    <a href=\"https://github.com/rasbot/Reinforcement_Learning_in_Unity/issues\">Reinforcement_Learning_in_Unityrt Bug</a>\n",
    "    ·\n",
    "    <a href=\"https://github.com/rasbot/Reinforcement_Learning_in_Unity/issues\">Request Feature</a>\n",
    "  </p>\n",
    "</p> -->\n",
    "\n",
    "\n",
    "\n",
    "<!-- TABLE OF CONTENTS -->\n",
    "## Table of Contents\n",
    "\n",
    "* [Introduction](#introduction)\n",
    "* [Reinforcement Learning Algorithms](#rl-algorithms)\n",
    "  * [PPO](#ppo)\n",
    "  * [SAC](#sac)\n",
    "* [Walking Trainer](#walking)\n",
    "  * [Untrained Walker](#untrained)\n",
    "  * [PPO Training](#ppotraining)\n",
    "  * [SAC Training](#sactraining)\n",
    "* [Contact](#contact)\n",
    "* [Acknowledgements](#acknowledgements)\n",
    "\n",
    "\n",
    "\n",
    "<!-- INTRODUCTION -->\n",
    "## Introduction\n",
    "---\n",
    "Reinforcement learning has many useful applications such as robotics and ... Researchers in artificial intelligence (AI) are constantly improving learning algorithms, and benefit from the ability to train and test their models in different environments. By enabling testing and training within a simulated environment, algorithms can be improved more rapidly. These ideal environments have a physics engine as well as graphics rendering. \n",
    "\n",
    "Machine learning (ML) models depend on what type of learning or predictions are involved.\n",
    "\n",
    "<img src=\"images/ML_map.PNG\" width=\"400\" height=\"200\"/>\n",
    "\n",
    "If the model will make a prediction of a specific target or label from a set of features, supervised learning can be used. If the data is unstructured or unlabeled, and the target feature is not known, unsupervised learning can be used to find relations between features. If the goal is to have a model learn from the environment through interaction, reinforcement learning is used.\n",
    "\n",
    "Reinforcement learning involves an __agent__, which could be a robot, a self-driving car, or a video game character, that interacts and learns from the __environment__. The agent observes the environment, takes __actions__, and receives __rewards__ based on those actions. The reward could be positive or negative, suchas if a robot moves closer to its target it would get a positive reward, and if it moves away from its target it would get a negative reward. The goal of the model is to maximize the reward to perform a specific task. An agent might not ever receive a positive reward but get penalized suchas an agent navigating a maze. The penalty might be a negative reward for each step and the model trains the agent to minimize the penalty. The algorithm used to train the model is called the __policy__. This will be discussed more later.\n",
    "\n",
    "<img src=\"images/agent_env.PNG\" width=\"500\" height=\"167\"/>\n",
    "\n",
    "The Unity game engine provides an editor which allows for ML algorithms to interact and learn from a variety of simulated environments. Recently the Unity team released version 1.0 of their [ML Agents toolkit](https://github.com/Unity-Technologies/ml-agents). This toolkit, along with the Unity editor, provides a user with the ability to create 3D simulated environments, a Python API to control and integrate ML algorithms into the environments, and a learning pipeline using C# to collect data from the environment, implement agent actions, and collect rewards. Agents can be trained and tested in the Unity engine using the ML agents toolkit.\n",
    "\n",
    "<img src=\"images/Unity_pipeline.PNG\" width=\"600\" height=\"450\"/>\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "The Unity Learning Environment\n",
    "</p>\n",
    "\n",
    "The Python API interfaces with the Unity game engine to create and control ML algorithms in the simulated environment. Python code initializes the environment variables, and feeds agent interactions to the model during training. The trained model is then fed back into the agent during testing.\n",
    "\n",
    "<!-- ALGORITHMS -->\n",
    "## Reinforcement Learning Algorithms\n",
    "---\n",
    "As mentioned above, the policy is the algorithm used in reinforcement learning. The simplest example, which doesn't involve any learning, is a stochastic policy where an agent randomly moves around an environment. If the goal is to do something like collect all the coins in an environment, a random walk will eventually get the job done. If the policy has feedback from the environment, a more intelligent policy can be adopted.\n",
    "\n",
    "\n",
    "<!-- PPO -->\n",
    "### PPO\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!-- SAC -->\n",
    "### SAC\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!-- WALKING TRAINER -->\n",
    "## Walking Trainer\n",
    "\n",
    "\n",
    "\n",
    "<!-- UNTRAINED WALKER -->\n",
    "### Untrained Walker\n",
    "<img src=\"images/UNTRAINED.gif\" width=\"600\" height=\"450\"/>\n",
    "<!-- PPO WALKER -->\n",
    "### PPO Training\n",
    "\n",
    "<!-- SAC WALKER -->\n",
    "### SAC Training\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/LANKY.gif\" width=\"600\" height=\"450\"/>\n",
    "\n",
    "<!-- CONTACT -->\n",
    "## Contact\n",
    "\n",
    "Nathan Rasmussen - nathan.f.rasmussen@gmail.com\n",
    "\n",
    "Project Link: [https://github.com/rasbot/Reinforcement_Learning_in_Unity](https://github.com/rasbot/Reinforcement_Learning_in_Unity)\n",
    "\n",
    "\n",
    "\n",
    "<!-- ACKNOWLEDGEMENTS -->\n",
    "## Acknowledgements\n",
    "\n",
    "* []()\n",
    "* []()\n",
    "* []()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!-- MARKDOWN LINKS & IMAGES -->\n",
    "<!-- https://www.markdownguide.org/basic-syntax/#reference-style-links -->\n",
    "\n",
    "<!-- [linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=flat-square&logo=linkedin&colorB=555\n",
    "[linkedin-url]: https://linkedin.com/in/nathanfrasmussen -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\gal\\\\Reinforcement_Learning_in_Unity'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnityEnvironmentException",
     "evalue": "Couldn't launch the /Builds/Walker environment. Provided filename does not match any environments.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnityEnvironmentException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-14b4c08ca4fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmlagents_envs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvironment\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mUnityEnvironment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# This is a non-blocking call that only loads the environment.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUnityEnvironment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"/Builds/Walker\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mside_channels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\natha\\miniconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\mlagents_envs\\environment.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, file_name, worker_id, base_port, seed, no_graphics, timeout_wait, args, side_channels)\u001b[0m\n\u001b[0;32m    196\u001b[0m             )\n\u001b[0;32m    197\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecutable_launcher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_graphics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m             logger.info(\n",
      "\u001b[1;32mc:\\users\\natha\\miniconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\mlagents_envs\\environment.py\u001b[0m in \u001b[0;36mexecutable_launcher\u001b[1;34m(self, file_name, no_graphics, args)\u001b[0m\n\u001b[0;32m    302\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_close\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m             raise UnityEnvironmentException(\n\u001b[1;32m--> 304\u001b[1;33m                 \u001b[1;34mf\"Couldn't launch the {file_name} environment. Provided filename does not match any environments.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m             )\n\u001b[0;32m    306\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnityEnvironmentException\u001b[0m: Couldn't launch the /Builds/Walker environment. Provided filename does not match any environments."
     ]
    }
   ],
   "source": [
    "from mlagents_envs.environment import UnityEnvironment\n",
    "# This is a non-blocking call that only loads the environment.\n",
    "env = UnityEnvironment(file_name=\"ml-agents-release_2/Builds/Walker\", seed=1, side_channels=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO:\n",
    "\n",
    "* Work on README:\n",
    "    * Intro to RL\n",
    "    \n",
    "    * Discuss Simulated environments\n",
    "    \n",
    "    * Describe types of RL:\n",
    "        \n",
    "        * Deep Q-Networks\n",
    "        \n",
    "        * CNN vs RNN\n",
    "        \n",
    "        * PPO and SAC\n",
    "    \n",
    "    * Show untrained model\n",
    "    \n",
    "    * Discuss hyperparameters\n",
    "    \n",
    "    * Show plots\n",
    "    \n",
    "    * Show trained model\n",
    "    \n",
    "    * Future work\n",
    "---\n",
    "\n",
    "* Get data for SAC runs\n",
    "\n",
    "* Get videos / gifs of untrained model, trained model, comparisons (race them)\n",
    "\n",
    "* Read book / unity paper to fill in details\n",
    "\n",
    "* Jupyter notebook with code to run training on exe file\n",
    "\n",
    "    * plots of everything\n",
    "    \n",
    "    * ???\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Reinforcement Learning\n",
    "\n",
    "* __Policy Gradients__\n",
    "\n",
    "* __Deep Q Networks (DQN)__\n",
    "\n",
    "* __Markov Decision Processes (MDP)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminology\n",
    "\n",
    "The AI (player) is the __agent__ which makes __observations__ within an __environment__, takes __actions__, and receives __rewards__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__policy__: The algorithm an agent uses to determine its actions. This can be a neural network, for example.\n",
    "\n",
    "    Stochastic Policy - A random algorithm suchas the one a robot vacuum uses\n",
    "    \n",
    "    Policy Search - search combinations of parameters, find the ones that maximizes performance\n",
    "    \n",
    "        * Brute force the search, checking all combinations\n",
    "        \n",
    "        * Genetic policy algorithm - create a random set of parameters, keep the 20% that perform\n",
    "        \n",
    "          the best from those, generate new sets...evolve the policy until it performs well\n",
    "          \n",
    "        * Evaluate the gradients of the rewards with regards to policy parameters\n",
    "          \n",
    "          (called policy gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/agent_env.PNG\" width=\"800\" height=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> REINFORCE algorithm\n",
    "\n",
    "> * Have AI play game several times, at each step, compute gradient but do not apply them\n",
    "\n",
    "> * Compute each action's advantage\n",
    "\n",
    "> * If advantage is positive (action is probably good), apply the gradients to make the action more likely\n",
    "\n",
    "> * Compute the mean of all resulting gradient vectors, use it to perform a Gradient Descent step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using RASCOM - Anaconda prompt with the (base) environment,\n",
    "\n",
    "* Navigate to `D:\\gal\\ml-agents`\n",
    "\n",
    "* Launch unity hub\n",
    "\n",
    "  * Launch project `Project` using Unity `2018.4.17f1`\n",
    " \n",
    "* Navigate to `ML-Agents/Examples/<pick an example>/Scenes/<name of scene>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walker training runs\n",
    "\n",
    "\n",
    "__Walker_T1__:\n",
    "behaviors:\n",
    "  Walker:\n",
    "    trainer: ppo\n",
    "    batch_size: 2048\n",
    "    beta: 0.005\n",
    "    buffer_size: 20480\n",
    "    epsilon: 0.2\n",
    "    hidden_units: 512\n",
    "    lambd: 0.95\n",
    "    learning_rate: 0.0003\n",
    "    learning_rate_schedule: linear\n",
    "    max_steps: 2e7\n",
    "    memory_size: 128\n",
    "    normalize: true\n",
    "    num_epoch: 3\n",
    "    num_layers: `3 -> 2`\n",
    "    time_horizon: 1000\n",
    "    sequence_length: 64\n",
    "    summary_freq: `30000 -> 3000`\n",
    "    use_recurrent: false\n",
    "    vis_encode_type: simple\n",
    "    reward_signals:\n",
    "      extrinsic:\n",
    "        strength: 1.0\n",
    "        gamma: 0.995\n",
    "        \n",
    "__Walker_T2__:\n",
    "behaviors:\n",
    "  Walker:\n",
    "    trainer: ppo\n",
    "    batch_size: 2048\n",
    "    beta: 0.005\n",
    "    buffer_size: 20480\n",
    "    epsilon: 0.2\n",
    "    hidden_units: 512\n",
    "    lambd: 0.95\n",
    "    learning_rate: `0.0003 -> 0.003`\n",
    "    learning_rate_schedule: linear\n",
    "    max_steps: `2e7 -> 6e3`\n",
    "    memory_size: 128\n",
    "    normalize: true\n",
    "    num_epoch: 3\n",
    "    num_layers: `3 -> 2`\n",
    "    time_horizon: 1000\n",
    "    sequence_length: 64\n",
    "    summary_freq: `30000 -> 3000`\n",
    "    use_recurrent: false\n",
    "    vis_encode_type: simple\n",
    "    reward_signals:\n",
    "      extrinsic:\n",
    "        strength: 1.0\n",
    "        gamma: 0.995\n",
    "        \n",
    "__Walker_T3__:\n",
    "behaviors:\n",
    "  Walker:\n",
    "    trainer: ppo\n",
    "    batch_size: 2048\n",
    "    beta: 0.005\n",
    "    buffer_size: 20480\n",
    "    epsilon: 0.2\n",
    "    hidden_units: 512\n",
    "    lambd: 0.95\n",
    "    learning_rate: `0.0003 -> 0.003`\n",
    "    learning_rate_schedule: linear\n",
    "    max_steps: `2e7 -> 6e3`\n",
    "    memory_size: 128\n",
    "    normalize: true\n",
    "    num_epoch: 3\n",
    "    num_layers: `3 -> 2`\n",
    "    time_horizon: 1000\n",
    "    sequence_length: 64\n",
    "    summary_freq: `30000 -> 3000`\n",
    "    use_recurrent: false\n",
    "    vis_encode_type: simple\n",
    "    reward_signals:\n",
    "      extrinsic:\n",
    "        strength: 1.0\n",
    "            gamma: 0.995"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTES__:\n",
    "\n",
    "> T1: Trained for 6ish hours\n",
    "\n",
    "> T2: Test to reduce max_steps, basically a dud\n",
    "\n",
    "> T3: First test of shorter runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised learning uses a static training set, whereas reinforcement learning generates the training data, which constantly changes, based on the agent's interaction with the environment. The data distribution of the observations and rewards are dynamic.\n",
    "\n",
    "Proximal Policy Optimization (PPO)\n",
    "Deep Q-networks store past experiences and update the policy based on them. PPO will learn from the direct interactions.\n",
    "\n",
    "\n",
    "Soft Actor-Critic (SAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create unity env\n",
    "from gym_unity.envs import BaseEnv\n",
    "env_id = \"Project\\Assets\\ML-Agents\\Builds\\UnityEnvironment.exe\"\n",
    "env = BaseEnv(env_id, worker_id=2, use_visual=False, no_graphics=False)\n",
    "\n",
    "# run stable baselines\n",
    "env = DummyVecEnv([lambda: env])  # The algorithms require a vectorized environment to run\n",
    "model = PPO2(MlpPolicy, env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlagents_envs.environment import UnityEnvironment\n",
    "# This is a non-blocking call that only loads the environment.\n",
    "env = UnityEnvironment(file_name=\"ml-agents-release_2/Builds/Walker\", seed=1, side_channels=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = MyChannel()\n",
    "env = UnityEnvironment(side_channels = [channel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "\n",
    "channel = EngineConfigurationChannel()\n",
    "\n",
    "env = UnityEnvironment(side_channels=[channel])\n",
    "\n",
    "channel.set_configuration_parameters(time_scale = 2.0)\n",
    "\n",
    "i = env.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
