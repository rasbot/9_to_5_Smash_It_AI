{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Reinforcement Learning\n",
    "\n",
    "* __Policy Gradients__\n",
    "\n",
    "* __Deep Q Networks (DQN)__\n",
    "\n",
    "* __Markov Decision Processes (MDP)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminology\n",
    "\n",
    "The AI (player) is the __agent__ which makes __observations__ within an __environment__, takes __actions__, and receives __rewards__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__policy__: The algorithm a agent uses to determine its actions. This can be a neural network, for example.\n",
    "\n",
    "    Stochastic Policy - A random algorithm suchas the one a robot vacuum uses\n",
    "    \n",
    "    Policy Search - search combinations of parameters, find the ones that maximizes performance\n",
    "    \n",
    "        * Brute force the search, checking all combinations\n",
    "        \n",
    "        * Genetic policy algorithm - create a random set of parameters, keep the 20% that perform\n",
    "        \n",
    "          the best from those, generate new sets...evolve the policy until it performs well\n",
    "          \n",
    "        * Evaluate the gradients of the rewards with regards to policy parameters\n",
    "          \n",
    "          (called policy gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/agent_env.PNG\" width=\"800\" height=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> REINFORCE algorithm\n",
    "\n",
    "> * Have AI play game several times, at each step, compute gradient but do not apply them\n",
    "\n",
    "> * Compute each action's advantage\n",
    "\n",
    "> * If advantage is positive (action is probably good), apply the gradients to make the action more likely\n",
    "\n",
    "> * Compute the mean of all resulting gradient vectors, use it to perform a Gradient Descent step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using RASCOM - Anaconda prompt with the (base) environment,\n",
    "\n",
    "* Navigate to `D:\\gal\\ml-agents`\n",
    "\n",
    "* Launch unity hub\n",
    "\n",
    "  * Launch project `Project` using Unity `2018.4.17f1`\n",
    " \n",
    "* Navigate to `ML-Agents/Examples/<pick an example>/Scenes/<name of scene>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walker training runs\n",
    "\n",
    "\n",
    "__Walker_T1__:\n",
    "behaviors:\n",
    "  Walker:\n",
    "    trainer: ppo\n",
    "    batch_size: 2048\n",
    "    beta: 0.005\n",
    "    buffer_size: 20480\n",
    "    epsilon: 0.2\n",
    "    hidden_units: 512\n",
    "    lambd: 0.95\n",
    "    learning_rate: 0.0003\n",
    "    learning_rate_schedule: linear\n",
    "    max_steps: 2e7\n",
    "    memory_size: 128\n",
    "    normalize: true\n",
    "    num_epoch: 3\n",
    "    num_layers: `3 -> 2`\n",
    "    time_horizon: 1000\n",
    "    sequence_length: 64\n",
    "    summary_freq: `30000 -> 3000`\n",
    "    use_recurrent: false\n",
    "    vis_encode_type: simple\n",
    "    reward_signals:\n",
    "      extrinsic:\n",
    "        strength: 1.0\n",
    "        gamma: 0.995\n",
    "        \n",
    "__Walker_T2__:\n",
    "behaviors:\n",
    "  Walker:\n",
    "    trainer: ppo\n",
    "    batch_size: 2048\n",
    "    beta: 0.005\n",
    "    buffer_size: 20480\n",
    "    epsilon: 0.2\n",
    "    hidden_units: 512\n",
    "    lambd: 0.95\n",
    "    learning_rate: `0.0003 -> 0.003`\n",
    "    learning_rate_schedule: linear\n",
    "    max_steps: `2e7 -> 6e3`\n",
    "    memory_size: 128\n",
    "    normalize: true\n",
    "    num_epoch: 3\n",
    "    num_layers: `3 -> 2`\n",
    "    time_horizon: 1000\n",
    "    sequence_length: 64\n",
    "    summary_freq: `30000 -> 3000`\n",
    "    use_recurrent: false\n",
    "    vis_encode_type: simple\n",
    "    reward_signals:\n",
    "      extrinsic:\n",
    "        strength: 1.0\n",
    "        gamma: 0.995\n",
    "        \n",
    "__Walker_T3__:\n",
    "behaviors:\n",
    "  Walker:\n",
    "    trainer: ppo\n",
    "    batch_size: 2048\n",
    "    beta: 0.005\n",
    "    buffer_size: 20480\n",
    "    epsilon: 0.2\n",
    "    hidden_units: 512\n",
    "    lambd: 0.95\n",
    "    learning_rate: `0.0003 -> 0.003`\n",
    "    learning_rate_schedule: linear\n",
    "    max_steps: `2e7 -> 6e3`\n",
    "    memory_size: 128\n",
    "    normalize: true\n",
    "    num_epoch: 3\n",
    "    num_layers: `3 -> 2`\n",
    "    time_horizon: 1000\n",
    "    sequence_length: 64\n",
    "    summary_freq: `30000 -> 3000`\n",
    "    use_recurrent: false\n",
    "    vis_encode_type: simple\n",
    "    reward_signals:\n",
    "      extrinsic:\n",
    "        strength: 1.0\n",
    "            gamma: 0.995"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTES__:\n",
    "\n",
    "> T1: Trained for 6ish hours\n",
    "\n",
    "> T2: Test to reduce max_steps, basically a dud\n",
    "\n",
    "> T3: First test of shorter runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised learning uses a static training set, whereas reinforcement learning generates the training data, which constantly changes, based on the agent's interaction with the environment. The data distribution of the observations and rewards are dynamic.\n",
    "\n",
    "Proximal Policy Optimization (PPO)\n",
    "Deep Q-networks store past experiences and update the policy based on them. PPO will learn from the direct interactions.\n",
    "\n",
    "\n",
    "Soft Actor-Critic (SAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create unity env\n",
    "from gym_unity.envs import BaseEnv\n",
    "env_id = \"Project\\Assets\\ML-Agents\\Builds\\UnityEnvironment.exe\"\n",
    "env = BaseEnv(env_id, worker_id=2, use_visual=False, no_graphics=False)\n",
    "\n",
    "# run stable baselines\n",
    "env = DummyVecEnv([lambda: env])  # The algorithms require a vectorized environment to run\n",
    "model = PPO2(MlpPolicy, env, verbose=1)\n",
    "model.learn(total_timesteps=10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
