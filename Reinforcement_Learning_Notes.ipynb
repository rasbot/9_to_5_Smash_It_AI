{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Reinforcement Learning\n",
    "\n",
    "* __Policy Gradients__\n",
    "\n",
    "* __Deep Q Networks (DQN)__\n",
    "\n",
    "* __Markov Decision Processes (MDP)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminology\n",
    "\n",
    "The AI (player) is the __agent__ which makes __observations__ within an __environment__, takes __actions__, and receives __rewards__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__policy__: The algorithm a agent uses to determine its actions. This can be a neural network, for example.\n",
    "\n",
    "    Stochastic Policy - A random algorithm suchas the one a robot vacuum uses\n",
    "    \n",
    "    Policy Search - search combinations of parameters, find the ones that maximizes performance\n",
    "    \n",
    "        * Brute force the search, checking all combinations\n",
    "        \n",
    "        * Genetic policy algorithm - create a random set of parameters, keep the 20% that perform\n",
    "        \n",
    "          the best from those, generate new sets...evolve the policy until it performs well\n",
    "          \n",
    "        * Evaluate the gradients of the rewards with regards to policy parameters\n",
    "          \n",
    "          (called policy gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/agent_env.PNG\" width=\"800\" height=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "# from tensorflow.python.keras.optimizers import Adam\n",
    "\n",
    "# tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02444914,  0.04753504, -0.02570328, -0.04110926])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02349844  0.24301596 -0.02652546 -0.34178972] 1.0 False {}\n"
     ]
    }
   ],
   "source": [
    "action = 1\n",
    "obs, reward, done, info = env.step(action)\n",
    "print(obs, reward, done, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_policy(obs): \n",
    "    angle = obs[2] \n",
    "    return 0 if angle < 0 else 1 \n",
    " \n",
    "totals = [] \n",
    "for episode in range(500): \n",
    "    episode_rewards = 0 \n",
    "    obs = env.reset() \n",
    "    for step in range(200): \n",
    "        action = basic_policy(obs) \n",
    "        obs, reward, done, info = env.step(action) \n",
    "        episode_rewards += reward \n",
    "        if done: \n",
    "            break \n",
    "    totals.append(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42.406, 8.392446842250477, 24.0, 71.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(totals), np.std(totals), np.min(totals), np.max(totals) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model only survived 72 steps (or 68, or whatever `np.max(totals)` gives). This is more of a brute force way to train the model, but the cart will have to move more erratically to keep up with the pole, and will ultimately fail. The neural net takes all 4 parameters, and outputs a probability. Since only two actions are possible (move left, or move right), the output gives $$p_{left} \\ and \\ p_{right}=(1-p_{left})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p_{left}$ is related to $action_0$ and $p_{right}$ is related to $action_1$\n",
    "\n",
    "If it outputs 0.7, then we pick action 0 with a 70% probability, and action 1 with a 30% probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/neural_net.PNG\" width=\"800\" height=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 4 # == env.observation_space.shape[0] \n",
    "keras.backend.clear_session()\n",
    "model = keras.models.Sequential([ \n",
    "    keras.layers.Dense(5, activation=\"elu\", input_shape=[n_inputs]), \n",
    "    keras.layers.Dense(1, activation=\"sigmoid\"), \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> REINFORCE algorithm\n",
    "\n",
    "> * Have AI play game several times, at each step, compute gradient but do not apply them\n",
    "\n",
    "> * Compute each action's advantage\n",
    "\n",
    "> * If advantage is positive (action is probably good), apply the gradients to make the action more likely\n",
    "\n",
    "> * Compute the mean of all resulting gradient vectors, use it to perform a Gradient Descent step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, obs, model, loss_fn): \n",
    "    with tf.GradientTape() as tape: \n",
    "        left_proba = model(obs[np.newaxis]) \n",
    "        action = (tf.random.uniform([1, 1]) > left_proba) \n",
    "        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32) \n",
    "        loss = tf.reduce_mean(loss_fn(y_target, left_proba)) \n",
    "    grads = tape.gradient(loss, model.trainable_variables) \n",
    "    obs, reward, done, info = env.step(int(action[0, 0].numpy())) \n",
    "    return obs, reward, done, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn): \n",
    "    all_rewards = [] \n",
    "    all_grads = [] \n",
    "    for episode in range(n_episodes): \n",
    "        current_rewards = [] \n",
    "        current_grads = [] \n",
    "        obs = env.reset() \n",
    "        for step in range(n_max_steps): \n",
    "            obs, reward, done, grads = play_one_step(env, obs, model, loss_fn) \n",
    "            current_rewards.append(reward) \n",
    "            current_grads.append(grads) \n",
    "            if done: \n",
    "                break \n",
    "        all_rewards.append(current_rewards) \n",
    "        all_grads.append(current_grads) \n",
    "    return all_rewards, all_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_factor): \n",
    "    discounted = np.array(rewards) \n",
    "    for step in range(len(rewards) - 2, -1, -1): \n",
    "        discounted[step] += discounted[step + 1] * discount_factor \n",
    "    return discounted \n",
    " \n",
    "def discount_and_normalize_rewards(all_rewards, discount_factor): \n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_factor) \n",
    "                              for rewards in all_rewards] \n",
    "    flat_rewards = np.concatenate(all_discounted_rewards) \n",
    "    reward_mean = flat_rewards.mean() \n",
    "    reward_std = flat_rewards.std() \n",
    "    return [(discounted_rewards - reward_mean) / reward_std \n",
    "            for discounted_rewards in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-22, -40, -50])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_rewards([10, 0, -50], discount_factor=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.28435071, -0.86597718, -1.18910299]),\n",
       " array([1.26665318, 1.0727777 ])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_and_normalize_rewards([[10, 0, -50], [10, 20]], discount_factor=0.8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 150 \n",
    "n_episodes_per_update = 10\n",
    "n_max_steps = 200 \n",
    "discount_factor = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(lr=0.01) \n",
    "loss_fn = keras.losses.binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-1634b245df32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     all_rewards, all_grads = play_multiple_episodes(\n\u001b[1;32m----> 6\u001b[1;33m         env, n_episodes_per_update, n_max_steps, model, loss_fn)\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mtotal_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_rewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m                     \u001b[1;31m# Not shown in the book\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     print(\"\\rIteration: {}, mean rewards: {:.1f}\".format(          # Not shown\n",
      "\u001b[1;32m<ipython-input-9-fee8dbc1a8d9>\u001b[0m in \u001b[0;36mplay_multiple_episodes\u001b[1;34m(env, n_episodes, n_max_steps, model, loss_fn)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_max_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplay_one_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[0mcurrent_rewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mcurrent_grads\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-bef8b87329fb>\u001b[0m in \u001b[0;36mplay_one_step\u001b[1;34m(env, obs, model, loss_fn)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_proba\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env.seed(42);\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    all_rewards, all_grads = play_multiple_episodes(\n",
    "        env, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
    "    total_rewards = sum(map(sum, all_rewards))                     # Not shown in the book\n",
    "    print(\"\\rIteration: {}, mean rewards: {:.1f}\".format(          # Not shown\n",
    "        iteration, total_rewards / n_episodes_per_update), end=\"\") # Not shown\n",
    "    all_final_rewards = discount_and_normalize_rewards(all_rewards,\n",
    "                                                       discount_rate)\n",
    "    all_mean_grads = []\n",
    "    for var_index in range(len(model.trainable_variables)):\n",
    "        mean_grads = tf.reduce_mean(\n",
    "            [final_reward * all_grads[episode_index][step][var_index]\n",
    "             for episode_index, final_rewards in enumerate(all_final_rewards)\n",
    "                 for step, final_reward in enumerate(final_rewards)], axis=0)\n",
    "        all_mean_grads.append(mean_grads)\n",
    "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
