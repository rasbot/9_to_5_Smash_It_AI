{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Reinforcement Learning\n",
    "\n",
    "* __Policy Gradients__\n",
    "\n",
    "* __Deep Q Networks (DQN)__\n",
    "\n",
    "* __Markov Decision Processes (MDP)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminology\n",
    "\n",
    "The AI (player) is the __agent__ which makes __observations__ within an __environment__, takes __actions__, and receives __rewards__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__policy__: The algorithm a agent uses to determine its actions. This can be a neural network, for example.\n",
    "\n",
    "    Stochastic Policy - A random algorithm suchas the one a robot vacuum uses\n",
    "    \n",
    "    Policy Search - search combinations of parameters, find the ones that maximizes performance\n",
    "    \n",
    "        * Brute force the search, checking all combinations\n",
    "        \n",
    "        * Genetic policy algorithm - create a random set of parameters, keep the 20% that perform\n",
    "        \n",
    "          the best from those, generate new sets...evolve the policy until it performs well\n",
    "          \n",
    "        * Evaluate the gradients of the rewards with regards to policy parameters\n",
    "          \n",
    "          (called policy gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/agent_env.PNG\" width=\"800\" height=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03814324, -0.03463322,  0.02370684, -0.00810226])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "env = gym.make('CartPole-v1')\n",
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03883591  0.16014087  0.0235448  -0.29321213] 1.0 False {}\n"
     ]
    }
   ],
   "source": [
    "action = 1\n",
    "obs, reward, done, info = env.step(action)\n",
    "print(obs, reward, done, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_policy(obs): \n",
    "    angle = obs[2] \n",
    "    return 0 if angle < 0 else 1 \n",
    " \n",
    "totals = [] \n",
    "for episode in range(500): \n",
    "    episode_rewards = 0 \n",
    "    obs = env.reset() \n",
    "    for step in range(200): \n",
    "        action = basic_policy(obs) \n",
    "        obs, reward, done, info = env.step(action) \n",
    "        episode_rewards += reward \n",
    "        if done: \n",
    "            break \n",
    "    totals.append(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42.616, 9.584390643123848, 24.0, 72.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(totals), np.std(totals), np.min(totals), np.max(totals) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model only survived 72 steps. This is more of a brute force way to train the model, but the cart will have to move more erratically to keep up with the pole, and will ultimately fail. The neural net takes all 4 parameters, and outputs a probability. Since only two actions are possible (move left, or move right), the output gives $$p_{left} \\ and \\ p_{right}=(1-p_{left})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p_{left}$ is related to $action_0$ and $p_{right}$ is related to $action_1$\n",
    "\n",
    "If it outputs 0.7, then we pick action 0 with a 70% probability, and action 1 with a 30% probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/neural_net.PNG\" width=\"800\" height=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras \n",
    " \n",
    "n_inputs = 4 # == env.observation_space.shape[0] \n",
    " \n",
    "model = keras.models.Sequential([ \n",
    "    keras.layers.Dense(5, activation=\"elu\", input_shape=[n_inputs]), \n",
    "    keras.layers.Dense(1, activation=\"sigmoid\"), \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
